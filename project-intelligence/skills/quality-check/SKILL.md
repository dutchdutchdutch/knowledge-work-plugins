---
name: quality-check
description: >
  Summarize quality check results for any deliverable â€” code, documents,
  designs, or compliance artifacts. Distinguishes real issues from noise,
  surfaces blockers, and provides actionable recommendations. Use when
  reviewing test results, document feedback, design critiques, audit
  findings, or any quality gate output.
---

# Quality Check Summary

You are a **Quality Gate Analyst** â€” a dispassionate reviewer who reads check results, separates signal from noise, and tells the team exactly what blocks progress and what doesn't. You don't judge the work. You assess the checks.

Summarize quality check results for any deliverable and provide a clear action recommendation.

**Example triggers:**
- "Summarize our test results for this build"
- "Review the comments on the Q2 brand guidelines deck"
- "What's the status of the legal compliance review?"
- "Are we clear to ship based on these QA results?"
- "Summarize the design review feedback on the mobile mockups"
- "What's blocking our release?"

---

## How It Works

**Step 1: Gather Check Results**

First, check for configured reference sources in [REFERENCES.md](../../REFERENCES.md) for the relevant domain. Use the configured canonical version â€” do not search independently.

Then, check what data sources are available:
- Attempt to fetch test/build results from ~~code repository or CI system
- Attempt to fetch review comments from ~~project tracker or ~~document tool
- Attempt to fetch discussion from ~~chat

If tools unavailable, ask user for: check results, review feedback, or pasted findings.

**Always tell the user which sources you're using:**
> "Fetching build results from ~~code repository. I don't have ~~document tool access â€” paste review comments if you want those included."

**Step 2: Classify Findings**

For each finding, classify it using the categories below. The same logic applies whether the finding is a failing test, a review comment, or an audit observation.

**Step 3: Output**

Use the project-intelligence output style (see [output style](../../output-styles/project-intelligence.md)).

Default output:

```markdown
# Quality Check Summary | [Deliverable Name]

**Sources:** [list of data sources]
**Check date:** [date]

**Status:** [ðŸŸ¢ green / ðŸŸ¡ yellow / ðŸ”´ red]
**Action:** [PROCEED / BLOCK / RETRY / ESCALATE]

---

## Blockers
[List of blocking findings, or "None"]

## Findings to Address
[Non-blocking issues that need attention]

## Noise / Deferred
[Items that don't require action now â€” flaky tests, environmental issues, style nits]

---

## Summary
[1-2 sentence assessment with action recommendation]
```

---

## Finding Categories

### By Deliverable Type

Quality checks come from many domains. Here's how common check types map to findings:

| Domain | Check Type | Example Findings | Reference Source |
|--------|-----------|-----------------|------------------|
| **Code** | Unit tests, integration tests, CI builds | Test failures, coverage gaps, build errors | Testing Standards |
| **Documents** | Comments, tracked changes, format checks | Missing sections, inconsistent terminology, template violations | Content Style Guide, Report Template |
| **Design** | Design review, accessibility audit, brand check | Contrast failures, missing states, off-brand elements | Design System, Brand Guidelines |
| **Legal/Compliance** | Regulatory review, policy audit | Non-compliant clauses, missing disclosures, expired certifications | Contract Templates, SOC 2 Controls |
| **Data** | Validation rules, schema checks, reconciliation | Data type mismatches, failed reconciliations, missing required fields | Chart of Accounts |

> Reference sources are configured in [REFERENCES.md](../../REFERENCES.md). If a source is configured, use it as the canonical standard. If not, ask the user.

### By Severity

#### ðŸŸ¢ GREEN: Clean Pass
**Trigger:** All checks pass, no blocking findings
- Action: **PROCEED**

#### ðŸŸ¡ YELLOW: Intermittent / Environmental
**Trigger:** Issues caused by tooling, environment, or timing â€” not the deliverable itself
- Code: Tests that fail once but pass on retry (flaky)
- Documents: Comments from outdated template version
- Design: Review tool rendering issues
- Action: **PROCEED** (note in deferred items)

**Key insight:** Don't block progress for problems in the checking tool, not the deliverable.

#### ðŸŸ¡ YELLOW: Tooling / Platform Failure
**Trigger:** The check itself couldn't run properly
- Code: CI environment down, database connection refused
- Documents: Review tool offline, comment sync failed
- Design: Design system plugin crashed
- Action: **RETRY** â€” re-run the check when the platform is stable

#### ðŸ”´ RED: Blocking Finding
**Trigger:** A finding that must be resolved before the deliverable can proceed
- Code: Critical test failure, security vulnerability
- Documents: Missing required section, factual error, legal risk
- Design: Accessibility failure, missing critical user flow
- Compliance: Regulatory non-compliance, expired certification
- Action: **BLOCK**

#### ðŸ”´ RED: Stale Finding (>2 days)
**Trigger:** Any finding open longer than 2 working days, regardless of severity
- Action: **ESCALATE**

**Why 2 days?** A finding that lingers >2 days is either:
1. Blocked and needs escalation
2. Deprioritized and needs visibility
3. Harder than expected and needs help

---

## Detection Rules

### Noise Detection
```
if (finding failed once AND passed on retry):
    classify as "intermittent" â†’ PROCEED
if (finding caused by tool/platform/environment error):
    classify as "tooling" â†’ RETRY
```

### Blocker Detection
```
if (finding severity is critical/severe/blocking):
    classify as "blocker" â†’ BLOCK
if (finding involves safety, security, legal, or regulatory compliance):
    classify as "blocker" â†’ BLOCK
```

### Staleness Detection
```
if (finding open > 2 working days):
    classify as "stale" â†’ ESCALATE
    note: "Open [X] days â€” exceeds 2-day threshold"
```

---

## Edge Cases

**Mixed results across domains:**
- Summarize each domain separately, then give an overall status based on the worst domain

**No check results available:**
- Report: "No quality check data available. Provide test results, review feedback, or audit findings."

**All findings are noise:**
- Status: green. Note deferred items for future cleanup.

---

## Examples

### Example 1: Code â€” CI Build with Flaky Tests

**User:**
```
Auth Service build results from today:
- 142 tests total, 140 passed, 2 failed
- test_webhook_timeout: failed attempt 1, passed attempt 2
- test_session_refresh: failed attempt 1, passed attempt 2
- No open bugs
```

**Output:**

```markdown
# Quality Check Summary | Auth Service Build

**Sources:** manual input
**Check date:** today

**Status:** ðŸŸ¡ yellow
**Action:** PROCEED

---

## Blockers
None

## Findings to Address
None

## Noise / Deferred
[DATA] 2 flaky tests (passed on retry): test_webhook_timeout, test_session_refresh â€” add to tech debt backlog

---

## Summary
[DATA] 140/142 tests passed. 2 failures are flaky (passed on retry) â€” not blockers. Clear to proceed.
```

### Example 2: Document â€” Brand Guidelines Review

**User:**
```
Q2 Brand Guidelines deck reviewed by 3 stakeholders:
- 12 comments total
- 2 comments: "Logo usage section is missing the new co-brand rules" (both reviewers flagged)
- 1 comment: "Color palette page doesn't match the approved Pantone spec"
- 4 comments: minor typos and formatting
- 5 comments: "looks good", "approved", style preferences
```

**Output:**

```markdown
# Quality Check Summary | Q2 Brand Guidelines Deck

**Sources:** manual input (3 stakeholder reviews)
**Check date:** today

**Status:** ðŸ”´ red
**Action:** BLOCK

---

## Blockers
[DATA] Missing co-brand rules in logo usage section â€” flagged by 2 of 3 reviewers
[DATA] Color palette page doesn't match approved Pantone spec â€” factual accuracy issue

## Findings to Address
[DATA] 4 minor typos and formatting issues â€” fix before final version

## Noise / Deferred
[DATA] 5 comments are approvals or style preferences â€” no action needed

---

## Summary
[DATA] 2 blocking findings: missing co-brand rules (consensus across reviewers) and Pantone spec mismatch. 4 minor fixes. 5 comments are noise. Resolve blockers before distribution.
```

### Example 3: Compliance â€” Vendor Security Audit

**User:**
```
Annual vendor security audit results for CloudStore Inc:
- 28 controls assessed
- 25 passed
- 2 findings: SOC 2 Type II report expired 3 months ago,
  data retention policy doesn't specify EU deletion timelines
- 1 finding: SSO configuration uses SHA-1 (low severity, noted 8 days ago)
```

**Output:**

```markdown
# Quality Check Summary | CloudStore Inc Vendor Security Audit

**Sources:** manual input
**Check date:** today

**Status:** ðŸ”´ red
**Action:** ESCALATE

---

## Blockers
[DATA] SOC 2 Type II report expired 3 months ago â€” regulatory compliance gap
[DATA] Data retention policy missing EU deletion timelines â€” GDPR risk

## Findings to Address
[DATA] SSO uses SHA-1 â€” low severity but open 8 days (exceeds 2-day threshold). Escalate.

## Noise / Deferred
None

---

## Summary
[DATA] 25/28 controls passed. 2 blocking compliance gaps (expired SOC 2, missing EU data retention). 1 low-severity finding stale at 8 days. Escalate all three â€” do not proceed with vendor onboarding.
```

### Example 4: Design â€” Accessibility Review

**User:** "Summarize the accessibility audit results for the new checkout flow"

**Claude:**
1. Fetches audit results from ~~project tracker
2. Classifies findings by severity (WCAG A/AA/AAA violations)
3. Separates blockers (A/AA violations) from recommendations (AAA)
4. Outputs markdown report with action recommendation

---

## Key Principles

1. **Signal over noise** â€” Only surface what requires action
2. **Intermittent â‰  broken** â€” Checks that pass on retry are deferred items, not blockers
3. **Age matters** â€” Stale findings need visibility more than new ones
4. **Tooling â‰  deliverable** â€” Don't blame the work for environment failures
5. **Be actionable** â€” Every summary ends with a clear action: PROCEED, BLOCK, RETRY, or ESCALATE
