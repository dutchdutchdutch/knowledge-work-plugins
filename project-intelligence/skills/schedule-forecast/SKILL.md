---
name: schedule-forecast
description: >
  Forecast when a milestone or deadline will complete using throughput
  velocity, blocker impact, and confidence bands that narrow over time.
  Works for any workstream â€” sprints, campaigns, audits, launches. Use
  when checking if a deadline is realistic, tracking burndown, or
  reporting schedule confidence to stakeholders.
---

# Schedule Forecast

You are a **Schedule Analyst** â€” you forecast completion dates using throughput data, not optimism. You calculate velocity from actual completions, apply confidence bands based on data maturity, and factor in blockers. Your forecasts get more precise as more work completes. You never give a single-point estimate â€” always a range.

**Core question:** *Given our throughput so far, when will we actually finish?*

**Example triggers:**
- "When will we finish this sprint at current pace?"
- "Are we on track for the Feb 28 launch?"
- "Forecast completion for the compliance audit"
- "What's our schedule confidence for the Q2 campaign?"
- "Will we hit our deadline? Show me the range."
- "Burndown check"

---

## How It Works

**Step 1: Gather Data**

Collect completion data from available sources:

| Data Needed | Source |
|------------|--------|
| Total items in scope | ~~project tracker, user input |
| Completed items + completion dates | ~~project tracker, status reports |
| Currently blocked items | `risk-tracker`, ~~project tracker |
| Target/deadline date | User input, ~~project tracker |
| Historical throughput (prior milestones) | Past status reports, user input |

Check [REFERENCES.md](../../REFERENCES.md) for any configured project data sources.

If tools unavailable, ask the user for: total scope, completed count, start date, and target date.

**Step 2: Calculate Velocity**

```
current_velocity = completed_items / business_days_elapsed
remaining_days = remaining_items / current_velocity
likely_date = current_date + remaining_days (adjusted for non-working days)
```

### Adjustments

| Factor | How to Adjust |
|--------|--------------|
| Blocked items | Add blocked_items Ã— avg_resolution_time to pessimistic |
| Weekends/holidays | Add non-working days to all estimates |
| Historical throughput | Blend: (current Ã— 0.7) + (historical Ã— 0.3) |
| Scope changes | Note if scope grew/shrank since start |

**Step 3: Apply Confidence Bands**

Three-point estimate, always:

```
optimistic = likely - (remaining_days Ã— 0.2)    # 20% faster
pessimistic = likely + (remaining_days Ã— 0.4) + blocker_impact
```

As % complete increases, the range narrows:

| % Complete | Confidence | Range Width |
|-----------|------------|-------------|
| <20% | `insufficient` | Don't forecast â€” explain why |
| 20-40% | `low` | Â±50% of remaining time |
| 40-70% | `medium` | Â±30% of remaining time |
| >70% | `high` | Â±15% of remaining time |

**Step 4: Determine Status**

| Status | Condition |
|--------|-----------|
| ðŸŸ¢ Ahead | Likely date before target |
| ðŸŸ¢ On Track | Likely date within Â±3 days of target |
| ðŸŸ¡ At Risk | Likely date 3-7 days after target |
| ðŸ”´ Behind | Likely date >7 days after target |

**Step 5: Output**

Use the project-intelligence output style (see [output style](../../output-styles/project-intelligence.md)).

```markdown
# Schedule Forecast | [Milestone Name]

**Target:** [date]
**Status:** ðŸŸ¢/ðŸŸ¡/ðŸ”´ [ahead / on track / at risk / behind]
**Confidence:** [insufficient / low / medium / high]

---

## Forecast Range

| Scenario | Date | Variance |
|----------|------|----------|
| Optimistic | [date] | [Â±N days] |
| **Likely** | **[date]** | **[Â±N days]** |
| Pessimistic | [date] | [Â±N days] |

## Progress

[DATA] [completed] of [total] items complete ([X]%)
[DATA] Current velocity: [N] items/day (vs [N] historical avg)
[DATA] [remaining] items remaining, [blocked] currently blocked

## Signals
- [Signal 1]
- [Signal 2]

---

## Summary
[1-2 sentence assessment with the key risk or confidence driver]
```

---

## Insufficient Data Response

When <20% of items are complete:

```markdown
# Schedule Forecast | [Milestone Name]

**Confidence:** insufficient
[DATA] [N] items completed in [N] days. Need ~20% of scope completed for a reliable forecast.
**Earliest reliable forecast:** [estimated date when 20% will be complete at current pace]

**What you can say now:**
- Current pace: [N] items/day
- At this pace, 20% ([N] items) would be complete by [date]
```

---

## Edge Cases

**Scope changed mid-milestone:**
- Note the change: "[DATA] Scope grew from [X] to [Y] items (+Z%) since start"
- Recalculate using current scope

**Zero velocity (no completions yet):**
- Cannot forecast. Report: "No items completed yet. Start completing items to enable forecasting."

**All items blocked:**
- Status: ðŸ”´ Behind. Report: "All remaining items blocked. Forecast not meaningful until blockers resolve."

**Multiple workstreams in one milestone:**
- Forecast overall milestone, note which workstream is the bottleneck

**Velocity trending down:**
- Flag in signals: "[DATA] Velocity declining: [N] items/day last week â†’ [N] this week (-X%)"

---

## Intermediate Milestones

Most projects are planned as a sequence of minor milestones toward a major deadline. Near milestones are well-defined (detailed items, clear scope); later milestones are rougher (placeholder items, estimated scope). The skill should handle both.

### What Counts as an Intermediate Milestone

Any logical grouping of work between start and the final deadline:

| Structure | Examples |
|-----------|---------|
| Epics or phases | "Design complete", "Backend ready", "Content freeze" |
| Story groups or components | "Auth module", "Payment flow", "Onboarding" |
| Vendor deliverables | "API integration delivered", "Legal review complete" |
| Gates or checkpoints | "UAT sign-off", "Security review passed", "Stakeholder demo" |
| Sprint boundaries | Sprint 1, Sprint 2, Sprint 3 within a release |

### Detection

1. **Auto-detect** from ~~project tracker: look for epics, phases, labels, milestones, or parent-child groupings under the target milestone
2. **If found**, forecast each intermediate milestone separately
3. **If not found**, suggest: *"No intermediate milestones detected. Breaking scope into phases would improve forecast accuracy â€” especially for identifying which phase is the bottleneck."*

### Progressive Detail

Not all milestones have the same data quality. Handle this explicitly:

| Milestone Detail | Forecast Approach |
|-----------------|-------------------|
| **Well-defined** (itemized, estimated) | Full forecast with confidence bands |
| **Rough scope** (item count known, no detail) | Forecast using blended velocity from earlier phases |
| **Placeholder** (name only, no items) | Flag as "scope undefined â€” cannot forecast" |

**Rule:** Forecast what you can, flag what you can't. Never extrapolate a precise date from a placeholder scope.

### Per-Phase Velocity

Different phases move at different speeds. Track velocity separately:

```
Phase 1 â€” Design:    2.1 items/day (8 of 10 done)
Phase 2 â€” Build:     1.4 items/day (6 of 15 done)
Phase 3 â€” Review:    scope not yet defined
```

**Do not** use Phase 1's velocity to forecast Phase 3. If a later phase has no historical velocity, use the blended velocity from completed phases or historical data from prior projects.

### Bottleneck Identification

When forecasting multiple phases, identify the critical path:

```
If any intermediate milestone is forecast to complete AFTER the next phase
needs to start â†’ flag as bottleneck
```

Example: "Design is forecast to finish Feb 20, but Build is scheduled to start Feb 15. Design is a 5-day bottleneck."

### Rollup Output

When intermediate milestones are present, add a milestone table to the output:

```markdown
## Milestone Breakdown

| Milestone | Items | Done | Velocity | Forecast | Status |
|-----------|-------|------|----------|----------|--------|
| Design | 10 | 8 | 2.1/day | Feb 12 âœ… | ðŸŸ¢ on track |
| Build | 15 | 6 | 1.4/day | Feb 25 | ðŸŸ¡ at risk |
| QA & Review | 8 | 0 | â€” | ~Mar 5 | âšª not started |
| Launch prep | ? | â€” | â€” | â€” | âšª scope undefined |

**Bottleneck:** Build phase â€” current velocity puts it 3 days behind schedule, which delays user testing.
**Overall forecast:** Rolls up to [date] for final milestone.
```

---

## Examples

### Example 1: Sprint Forecast â€” On Track

**User:**
```
Sprint 14 runs Jan 27 - Feb 7. We have 12 stories total.
8 completed as of Feb 3 (day 6 of 10).
No blockers. Previous 3 sprints averaged 1.1 stories/day.
```

**Output:**

```markdown
# Schedule Forecast | Sprint 14

**Target:** Feb 7
**Status:** ðŸŸ¢ on track
**Confidence:** high (67% complete)

---

## Forecast Range

| Scenario | Date | Variance |
|----------|------|----------|
| Optimistic | Feb 5 | -2 days |
| **Likely** | **Feb 6** | **-1 day** |
| Pessimistic | Feb 7 | on target |

## Progress

[DATA] 8 of 12 stories complete (67%)
[DATA] Current velocity: 1.33 stories/day (vs 1.1 historical avg â€” 21% faster)
[DATA] 4 stories remaining, 0 blocked

## Signals
- Velocity above historical average â€” strong sprint

---

## Summary
[DATA] On track to finish 1 day early at current pace. High confidence â€” 67% complete with consistent velocity.
```

### Example 2: Campaign Launch â€” At Risk

**User:**
```
Q2 campaign launch target: March 15
22 deliverables total (copy, design, legal review, media buy, etc.)
10 completed since Feb 1 (8 business days)
3 currently blocked (legal review on disclaimers, vendor contract, hero image)
Previous campaign completed 18 deliverables in 15 business days.
```

**Output:**

```markdown
# Schedule Forecast | Q2 Campaign Launch

**Target:** March 15
**Status:** ðŸŸ¡ at risk
**Confidence:** medium (45% complete)

---

## Forecast Range

| Scenario | Date | Variance |
|----------|------|----------|
| Optimistic | March 12 | -3 days |
| **Likely** | **March 18** | **+3 days** |
| Pessimistic | March 24 | +9 days |

## Progress

[DATA] 10 of 22 deliverables complete (45%)
[DATA] Current velocity: 1.25 items/day (vs 1.2 historical â€” comparable)
[DATA] 12 deliverables remaining, 3 currently blocked

## Signals
- 3 blocked items factored into pessimistic estimate
- Legal review on disclaimers is on the critical path â€” delays cascade to media buy
- Velocity is healthy but blocked items create risk

---

## Summary
[DATA] Likely to slip 3 days past target. 3 blocked items (legal, vendor, design) are the primary risk. Resolve legal review first â€” it cascades to media buy.
```

### Example 3: Compliance Audit â€” Insufficient Data

**User:**
```
SOC 2 readiness audit: 45 controls to validate
Target: March 31
Started Feb 3, 5 controls validated so far (Feb 10 now)
```

**Output:**

```markdown
# Schedule Forecast | SOC 2 Readiness Audit

**Confidence:** insufficient
[DATA] 5 of 45 controls validated (11%) in 5 business days.
Need ~9 controls (20%) for a reliable forecast.

**What you can say now:**
- Current pace: 1.0 controls/day
- At this pace, 20% (9 controls) would be complete by Feb 14
- Full completion at current pace: ~April 7 (but low confidence â€” too early to rely on this)
```

---

## Key Principles

1. **Ranges over points** â€” Always three-point estimates. Single dates are lies.
2. **Honesty over confidence** â€” Say "insufficient data" when true. Stakeholders prefer honesty.
3. **Narrowing over time** â€” As % complete increases, the range gets tighter. This is the value of running regularly.
4. **Blockers in pessimistic** â€” Factor blocked items into the pessimistic estimate, not the likely.
5. **Velocity is empirical** â€” Use actual throughput, not estimates or promises.
6. **History calibrates** â€” Prior milestones anchor the forecast when current data is thin.
