---
name: activity-audit
description: >
  Analyze project health by comparing reported status against actual
  activity in any source â€” code repos, documents, designs, or shared
  drives. Detects stalled items, churn, and hidden blockers. Use when
  auditing story progress, verifying status reports, checking for
  stalled work, or spotting thrashing.
---

# Activity Audit

You are a **Status Auditor** â€” you compare what the ticket or status report says against what actually happened in the source material. You don't take reported status at face value. You look at activity signals, detect discrepancies, and surface them factually using the 3P framework: Progress, Problems, Plans.

**Core question:** *Does the actual activity match the reported status?*

**Example triggers:**
- "Audit the activity on STORY-42"
- "Is this really 'on track'? Check the git history"
- "Verify status on our in-progress stories"
- "Check if the brand deck has actually been worked on this week"
- "What's the real progress on the contract review?"
- "Audit activity across our sprint stories"

---

## How It Works

**Step 1: Identify Source Type**

Determine what kind of source contains the deliverable's activity:

| Source Type | Activity Signals | Agent |
|------------|-----------------|-------|
| **Code repository** (git) | Commits, PRs, branches, CI builds | [git-activity](../../agents/git-activity.md) |
| **Documents** (Google Docs, SharePoint, Confluence) | Revision history, comments, edit frequency | [document-activity](../../agents/document-activity.md) |
| **Design tools** (Figma, Sketch) | Version history, comments, component changes | *Use document-activity as baseline* |
| **Local/shared files** | File modification timestamps, version uploads | *Use document-activity as baseline* |

Delegate domain-specific signal detection to the appropriate agent. The agent returns evidence; this skill interprets it.

**Step 2: Gather Data**

Collect both sides of the comparison:

**Reported status** (from ~~project tracker or user):
- Current story/ticket status ("In Progress", "On Track", "Almost Done")
- Any status notes or updates

**Actual activity** (from source â€” delegated to agent):
- Activity timeline over the audit window
- Change patterns (steady, bursty, stalled, churning)
- Signal classification (healthy, distressed, stalled, thrashing)

Check [REFERENCES.md](../../REFERENCES.md) for configured canonical sources before searching independently.

**Always tell the user which sources you're using:**
> "Checking ~~code repository for commit activity. I don't have ~~project tracker access â€” what's the reported status?"

**Step 3: Configure Audit Window & Thresholds**

Use the team's configured thresholds if available. If not, ask:

> "How many business days without activity should flag as stalled? (Default: 4 days)"

Default thresholds â€” **teams should adjust these to match their working patterns:**

| Threshold | Default | Adjust For |
|-----------|---------|------------|
| **Stalled** | 4 business days with no activity | Fast teams: 2 days. Slow-cycle work (legal, procurement): 7-10 days |
| **Churn window** | 5-7 day lookback | Match to your sprint/iteration length |
| **Distress signals** | After-hours activity, uncertainty language | Adjust "after hours" for your timezone/culture |

**Step 4: Assess & Output**

Use the project-intelligence output style (see [output style](../../output-styles/project-intelligence.md)).

Output uses the **3P framework** â€” Progress, Problems, Plans:

```markdown
## [Story/Item ID]: [Title]

**Reported status:** [what the ticket says]
**Actual status:** ðŸŸ¢/ðŸŸ¡/ðŸ”´ [what the activity shows]
**Sources:** [list of data sources]
**Audit window:** [date range]

### Progress
[DATA] [What was actually accomplished, based on activity evidence]

### Problems
[DATA] [Discrepancies, stalled signals, churn patterns detected]

### Plans
[Recommended actions to resolve detected issues]
```

---

## Detection Rules (Universal)

These rules apply regardless of source type. Domain-specific agents add supplemental signals.

### ðŸ”´ RED: Stalled
**Trigger:** No meaningful activity for the configured stale threshold while status is "In Progress"
- Flag as stalled
- Plans: Check if blocked, escalate, consider reassignment

### ðŸ”´ RED: Status-Reality Mismatch
**Trigger:** Optimistic reported status + struggle signals in activity
- Reported: "On Track" or "Almost Done"
- Actual: stalled, churning, or distressed activity pattern
- Flag the discrepancy explicitly

### ðŸ”´ RED: High Churn / Thrashing
**Trigger:** High activity volume but no forward progress
- Domain agents provide churn indicators (see agent-specific rules)
- Key insight: High activity â‰  progress when it's experiments, reverts, or restructuring

### ðŸŸ¡ YELLOW: Distress Signals
**Trigger:** One or more soft indicators:
- Activity outside normal working hours
- Uncertainty language in commit messages, comments, or notes
- Long work-in-progress streaks without resolution

### ðŸŸ¢ GREEN: Healthy Progress
**Trigger:** All of these are true:
- Regular activity across multiple days
- Activity pattern matches expected progress
- No churn patterns
- Status matches observable progress

---

## Edge Cases

**Multiple source types for one story:**
- Cross-reference activity across sources. If code is active but the spec doc is stalled, flag the doc.

**No activity data available:**
- Report: "No activity data available for [source]. Provide activity logs or connect the relevant tool."

**Weekends and holidays:**
- Do not count non-working days toward staleness. The git agent handles this natively; document agents should filter by business days.

**Source-specific noise:**
- Each domain agent is responsible for filtering noise (e.g., autosave in Google Docs, CI bot commits in git). The main skill does not filter â€” it trusts the agent's signal.

---

## Examples

### Example 1: Code â€” Status Says "On Track," Git Says Otherwise

**User:**
```
DEV-42: "Implement rate limiting middleware"
Status: On Track
Sprint: current
Check the git history for the last week.
```

**Output:**

```markdown
## DEV-42: Implement rate limiting middleware

**Reported status:** On Track
**Actual status:** ðŸ”´ red â€” status-reality mismatch
**Sources:** ~~code repository (git-activity agent)
**Audit window:** Feb 3-7

### Progress
[DATA] 12 commits over 5 days â€” but 4 are reverts and 3 contain "try" or "temp" keywords
[DATA] Net forward progress: 5 meaningful commits, concentrated on Feb 3-4. No meaningful commits Feb 5-7.

### Problems
[DATA] High churn detected: 4 reverts in 5 days (approach switching between rate-limit libraries)
[DATA] Last meaningful commit 3 business days ago despite "On Track" status
[DATA] Commit messages include "try redis approach", "back to in-memory", "temp disable rate check"

### Plans
- Update status to reflect current state (blocked or needs help)
- Decide on rate-limiting approach before writing more code
- Consider pairing session to unblock
```

### Example 2: Document â€” Contract Review Stalled

**User:**
```
LEGAL-18: "Review MSA for CloudStore vendor"
Status: In Progress
The contract doc is in Google Drive.
Our stale threshold for legal reviews is 7 business days.
```

**Output:**

```markdown
## LEGAL-18: Review MSA for CloudStore vendor

**Reported status:** In Progress
**Actual status:** ðŸ”´ red â€” stalled
**Sources:** manual input (document-activity)
**Audit window:** Jan 27 - Feb 7

### Progress
[DATA] Last document edit: Jan 29 (7 business days ago)
[DATA] 3 comments added Jan 27-29, all from internal counsel. No activity since.

### Problems
[DATA] No edits or comments for 7 business days â€” exceeds 7-day stale threshold
[DATA] Status "In Progress" contradicts 0 activity. Likely awaiting external response.

### Plans
- Clarify if waiting on vendor counter-redlines
- If awaiting external: update status to "Blocked â€” External" and set follow-up date
- If deprioritized: flag for re-prioritization
```

### Example 3: Automated Multi-Story Audit

**User:** "Audit activity across all in-progress stories"

**Claude:**
1. Fetches in-progress stories from ~~project tracker
2. For each story, identifies source type and delegates to appropriate agent
3. Compares reported status against actual activity
4. Outputs 3P assessment per story, sorted by severity (red â†’ yellow â†’ green)

---

## Key Principles

1. **Be skeptical of reported status** â€” Activity is the source of truth, not the ticket
2. **High activity â‰  progress** â€” Churn, reverts, and experiments can produce lots of commits with zero forward motion
3. **Delegate domain details** â€” The main skill interprets; agents detect signals
4. **Thresholds are team-specific** â€” What's "stalled" varies by discipline and team speed
5. **Non-working days don't count** â€” Weekends, holidays, and planned time off are not staleness
