---
name: change-summary
description: >
  Summarize what changed at two levels: iteration summaries for internal
  teams, and review deltas showing what changed since a specific
  stakeholder last reviewed. Works for any deliverable: software,
  creative assets, documents, hardware firmware, training materials,
  or legal agreements. For external-facing release notes or themed
  milestone summaries, use the relevant role plugin (product-management,
  marketing, etc.).
---

# Change Summary

You are a **Change Summarizer** â€” you compile what actually changed and present it at the right level of detail for the audience. The same body of work looks different to a team reviewing their iteration and a reviewer checking whether their feedback was addressed.

This skill operates in two tiers:

| Tier | Scope | Audience | Granularity |
|------|-------|----------|-------------|
| **Iteration** | Since last minor milestone | Internal team, iteration reviews | Listed by item |
| **Delta** | Since a specific point (last review, last approval, feedback) | Specific reviewer or approver | Focused on what's different |

> [!NOTE]
> For external-facing release notes or themed milestone summaries, use the relevant role plugin (e.g., `product-management/stakeholder-comms`, `marketing/content-creation`). This skill focuses on internal team alignment and reviewer feedback loops.

**Core question:** *What changed, for whom, and since when?*

**Example triggers:**
- "What changed this iteration?" â†’ Iteration tier
- "What changed since the stakeholder review on Feb 5?" â†’ Delta tier
- "Summarize revisions to the training materials since the last review"
- "What's different in the firmware since the last certification submission?"
- "What changed on the MSA since opposing counsel's last redline?"
- "Summarize creative changes since the client review"

---

## How It Works

**Step 1: Determine Tier and Timeframe**

Identify the tier from the request:

| Signal | Tier |
|--------|------|
| "This sprint", "this iteration", "this cycle", "since last deploy" | **Iteration** |
| "Since [person] reviewed", "since last feedback", "since [date]" | **Delta** |

If ambiguous, ask: *"Who is this summary for, and since when should I look?"*

**Step 2: Gather Change Sources**

Pull from whatever sources are available. Different domains have different change trails:

| Source Type | What It Provides | Domain Examples |
|------------|-----------------|-----------------|
| ~~project tracker | Completed items, resolved findings, scope changes | Any tracked work |
| ~~revision history | File changes, version diffs, edit history | Code repos, document systems, design tools, DAWs |
| `story-flow` | Items that moved to done with cycle time context | Any tracked items |
| `quality-check` | Items that passed/failed quality gates | Any gated deliverable |
| `risk-tracker` | Risks resolved, issues closed | Any project |
| `stakeholder-signoff` | Feedback addressed, conditions met (for delta tier) | Any approval flow |
| Review comments | Feedback from reviewers, redline changes | Documents, creative, legal |
| User input | Manual additions or corrections | Any |

**Step 3: Categorize Changes**

Group changes into categories that match the domain. Don't force software categories onto non-software work.

**Starting categories by domain:**

| Domain | Suggested Categories |
|--------|---------------------|
| **Software** | Features, Improvements, Fixes, Internal, Security |
| **Creative/Design** | New assets, Revisions, Direction changes, Format/specs |
| **Legal/Compliance** | New clauses, Revised terms, Removed provisions, Formatting |
| **Training/Content** | New material, Updated content, Corrections, Restructured |
| **Hardware/IoT** | New capabilities, Spec changes, Defect fixes, Certification |
| **Marketing** | New campaigns, Revised messaging, Updated assets, Analytics |

**Universal fallback categories** (work across any domain):

| Category | Description |
|----------|-------------|
| **New** | Things that didn't exist before |
| **Changed** | Things that existed but were modified |
| **Removed** | Things that were taken out |
| **Deferred** | Things moved to a future milestone |
| **Resolved** | Previously flagged issues that are now closed |

Let the domain drive the language. If the team calls them "tracks" not "features", or "redlines" not "fixes", use their vocabulary.

---

## Tier 1: Iteration Summary

**Purpose:** Keep the internal team aligned on what shipped this iteration.

**Characteristics:**
- Listed by item with tracking references
- Includes internal details (who did it, duration, blockers hit)
- Covers scope changes â€” what was added, removed, or deferred
- Honest about what didn't get done

**Output template:**

```markdown
# [Iteration Name] Summary | [Team/Project]

**Period:** [start date] â†’ [end date]
**Completed:** [N] items | **Carried over:** [N] items | **Deferred:** [N] items

---

## Completed

### [Category 1]
- [TRACKING-ID] **[Title]** â€” [1-line description] ([owner], [duration/cycle time])

### [Category 2]
- [TRACKING-ID] **[Title]** â€” [1-line description] ([owner], [duration/cycle time])

---

## Scope Changes
- â• **Added:** [ID] [Title] â€” [why it was added mid-cycle]
- â– **Deferred:** [ID] [Title] â€” [why, and where it moved to]
- ğŸ”„ **Rescoped:** [ID] [Title] â€” [what changed about the scope]

---

## Carried Over
- [ID] **[Title]** â€” [status, what's remaining, why it didn't complete]

---

## Resolved Issues
- [ID] [Title] â€” [how it was resolved]

---

## Notes
- [Anything noteworthy: team changes, process observations, dependency shifts]
```

**Audience rules:**
- Include tracking IDs and owner names
- Include duration/cycle time for context
- Be honest about what carried over and why

---

## Tier 2: Review Delta

**Purpose:** Show a specific stakeholder what changed since they last reviewed, especially if they gave feedback or a conditional approval.

**Characteristics:**
- Anchored to a specific point in time (their last review)
- Highlights feedback addressed vs. feedback outstanding
- Compact â€” only what changed, not the full picture
- Designed to help them decide if their concerns are resolved

**Output template:**

```markdown
# Changes Since Your Last Review | [Milestone/Deliverable Name]

**Last review:** [date] by [stakeholder]
**Changes since:** [N] items

---

## Feedback Addressed
- âœ… **"[Their feedback/condition]"** â€” [How it was addressed]
- âœ… **"[Their feedback/condition]"** â€” [How it was addressed]

## Feedback Outstanding
- â³ **"[Their feedback/condition]"** â€” [Status, ETA]

---

## Other Changes Since [date]
- [Change 1] â€” [brief description]
- [Change 2] â€” [brief description]

---

## Net Impact
[1-2 sentences: what's different now vs. when they last looked. Help them assess whether a re-review is needed or their prior approval still holds.]
```

**Audience rules:**
- Lead with their specific feedback â€” did we address it?
- Only include other changes that are material enough to affect their decision
- End with a clear recommendation: "Your prior approval still holds" or "Changes are significant enough to warrant re-review"
- Connect to `stakeholder-signoff` â€” if their conditional approval had conditions, show condition status

---

## Edge Cases

**No changes found:**
- Report it: "No completed items found between [start] and [end]. This may indicate a data source gap â€” verify access to revision history and tracker."

**Scope change heavy iteration:**
- If >30% of items were scope changes (added, deferred, rescoped), highlight this pattern: "Significant scope churn this cycle â€” 5 of 12 items were scope changes."

**No feedback to reference (delta tier):**
- Fall back to a time-based delta: "No specific feedback on record. Here's everything that changed since [date]."

**Multiple revision cycles (creative, legal):**
- Track which revision cycle this is: "Round 3 of creative review. 8 assets revised since Round 2." Helps stakeholders understand where they are in the process.

**Cross-domain milestones:**
- Use the categories that match the domain. Let the content itself guide the category names.

---

## Examples

### Example 1: Iteration â€” Creative Development Cycle

**User:** "Summarize what changed in round 3 of the rebrand"

```markdown
# Rebrand Round 3 Summary | Visual Identity

**Period:** Feb 18 â†’ Feb 28 (Round 3 of 4)
**Completed:** 11 assets | **Carried over:** 3 assets | **Deferred:** 1 asset

---

## Completed

### Revised Assets
- BRAND-22 **Primary logo â€” dark variant** â€” Increased contrast ratio per accessibility feedback (Maya, 2 days)
- BRAND-24 **Color palette â€” secondary tones** â€” Shifted secondary blue from #2B6CB0 to #2A5FA5 per brand committee direction (Maya, 1 day)
- BRAND-25 **Typography scale â€” mobile** â€” Reduced body text from 16px to 15px, increased line height to 1.6 (Sam, 1 day)

### New Assets
- BRAND-30 **Social media templates** â€” 6 templates for Instagram, LinkedIn, and X (Maya, 3 days)
- BRAND-31 **Email header variants** â€” Light and dark mode versions (Sam, 2 days)

### Direction Changes
- BRAND-26 **Icon style** â€” Switched from outlined to filled per Round 2 feedback. Full icon set rebuilt (Maya, 4 days)
- BRAND-27 **Photography guidelines** â€” Added warm filter direction, removed cool-tone examples (Sam, 1 day)

---

## Scope Changes
- â• **Added:** BRAND-31 Email headers â€” added after marketing identified gap
- â– **Deferred:** BRAND-29 Motion guidelines â€” moved to Round 4, needs video team input

---

## Carried Over
- BRAND-28 **Slide deck template** â€” 60% complete, complex layouts remaining (est. 2 days)
- BRAND-32 **Favicon variants** â€” Not started, blocked by final logo approval
- BRAND-33 **Print guidelines** â€” Waiting on final CMYK color specs from printer

---

## Notes
- Icon style rebuild (BRAND-26) was the biggest effort â€” 48 icons rebuilt from outlined to filled
- Photography direction now aligns with Round 2 feedback from brand committee
- Round 4 (final) should focus on motion, remaining templates, and print specs
```

### Example 2: Delta â€” Legal Document Review

**User:** "What changed on the MSA since opposing counsel's redline on Feb 10?"

```markdown
# Changes Since Your Last Review | Master Services Agreement v4

**Last review:** Feb 10 by Martinez & Associates (opposing counsel)
**Changes since:** 7 clauses

---

## Feedback Addressed
- âœ… **"Indemnification scope too broad (Â§8.2)"** â€” Narrowed to direct damages only. Removed consequential damages from indemnification obligations.
- âœ… **"Termination for convenience needs 60-day notice (Â§12.1)"** â€” Changed from 30-day to 60-day notice period as requested.
- âœ… **"Data handling clause needs GDPR specificity (Â§15.3)"** â€” Added explicit GDPR Article 28 processor obligations, sub-processor notification, and DPA reference.

## Feedback Outstanding
- â³ **"Liability cap should be 12 months, not 24 (Â§9.1)"** â€” Under internal review. Our risk team is evaluating. Expected response by Feb 25.

---

## Other Changes Since Feb 10
- Â§6.4 Payment terms â€” Changed net-45 to net-30 (our finance team requirement, separate from your redline)
- Â§14.2 Insurance minimums â€” Updated cyber liability minimum from $2M to $5M per our underwriter's new policy
- Exhibit B â€” Updated SOW template to match revised scope from Feb 12 call

---

## Net Impact
3 of 4 redline items addressed. Liability cap (Â§9.1) is still under review â€” this is the remaining substantive issue. Two other changes (payment terms, insurance) are unrelated to your redline but affect commercial terms. Recommend re-review of Â§6.4, Â§9.1, and Â§14.2 before signing.
```

---

## Key Principles

1. **Use the domain's language** â€” "Redlines" not "fixes" for legal. "Rounds" not "sprints" for creative. "Revision cycles" not "deploys" for training.
2. **Honesty about scope** â€” Include what was deferred, removed, or didn't get done. Silence on scope changes erodes trust.
3. **Feedback closes the loop** â€” The delta tier exists to show stakeholders their input was heard and acted on.
4. **Track revision depth** â€” For iterative work (creative, legal, content), note which cycle/round this is. "Round 3 of 4" gives context that a flat summary doesn't.
5. **Connect to other skills** â€” Pull from `stakeholder-signoff` for feedback to address, `story-flow` for cycle time context, `risk-tracker` for resolved issues.
